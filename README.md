# Dataset Distillation Using Auto-Encoders

## Введение в Dataset Disstilation
Наша задача заключается в том, чтобы уместить всю информацию об обучающей выборке, обычно состоящей из тысячи до миллиона изображений, в небольшое количество дистиллированных изображений. Для инициализации дистиллированных изображений мы используем различные автоэнкодеры и учим их латентные представления, с целью получить дистиллированные изображения наиболее схожие с изображениями из обучающей выборки. Этот код является расширением статьи [Dataset Distillation](https://ssnl.github.io/dataset_distillation/) и все еще дорабатывается.

## Использование 
* Использование pre_trained сверточного автоэнкодеров для синтеза дистиллированных изображений.
```
python main.py --pre_trained_decoder --conv_decoder
```
* Использование pre_trained GAN-генератора для синтеза дистиллированных изображений и обучения логитов.
```
python main.py --pre_trained_decoder --gan_generator --learn_labels
```

